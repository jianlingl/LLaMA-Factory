nohup: ignoring input
[INFO|2024-11-27 13:35:44] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:27842
W1127 13:35:58.757000 3708535 site-packages/torch/distributed/run.py:793] 
W1127 13:35:58.757000 3708535 site-packages/torch/distributed/run.py:793] *****************************************
W1127 13:35:58.757000 3708535 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1127 13:35:58.757000 3708535 site-packages/torch/distributed/run.py:793] *****************************************
[WARNING|2024-11-27 13:37:07] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2024-11-27 13:37:07,167 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/config.json
[INFO|configuration_utils.py:746] 2024-11-27 13:37:07,184 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,212 >> loading file tokenizer_config.json
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2024-11-27 13:37:07] llamafactory.hparams.parser:355 >> Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2024-11-27 13:37:07,703 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2024-11-27 13:37:07,714 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/config.json
[INFO|configuration_utils.py:746] 2024-11-27 13:37:07,714 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2024-11-27 13:37:07,728 >> loading file tokenizer_config.json
[rank3]:[W1127 13:37:08.921951367 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W1127 13:37:08.949940730 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[INFO|tokenization_utils_base.py:2475] 2024-11-27 13:37:08,159 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2024-11-27 13:37:08] llamafactory.data.template:157 >> Replace eos token: <|im_end|>
[INFO|2024-11-27 13:37:08] llamafactory.data.loader:157 >> Loading dataset /mnt/afs/code_corpus/triton_corpus/synthe_train_alpac.json...
[rank6]:[W1127 13:37:08.107340634 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W1127 13:37:08.139902427 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W1127 13:37:08.145518938 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W1127 13:37:08.146672002 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1127 13:37:08.154762499 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 0/4133 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 259/4133 [00:00<00:02, 1655.85 examples/s]Converting format of dataset (num_proc=16):  38%|███▊      | 1551/4133 [00:00<00:00, 6546.78 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 4133/4133 [00:00<00:00, 10166.67 examples/s]
[INFO|2024-11-27 13:37:09] llamafactory.data.loader:157 >> Loading dataset /mnt/afs/code_corpus/triton_corpus/crawl_train_compInstru_alpac.json...
Converting format of dataset (num_proc=16):   0%|          | 0/3415 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  17%|█▋        | 594/3415 [00:00<00:00, 5273.45 examples/s]Converting format of dataset (num_proc=16):  36%|███▌      | 1236/3415 [00:00<00:00, 5727.30 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 3415/3415 [00:00<00:00, 8996.11 examples/s]
[rank0]:[W1127 13:37:10.815213757 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/7548 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 472/7548 [00:02<00:33, 212.09 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 944/7548 [00:02<00:14, 463.68 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 1416/7548 [00:02<00:08, 751.48 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 1888/7548 [00:02<00:06, 920.82 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 2360/7548 [00:03<00:04, 1230.59 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 2832/7548 [00:03<00:03, 1460.41 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 3304/7548 [00:03<00:02, 1685.37 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 3776/7548 [00:03<00:01, 2074.97 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 4248/7548 [00:03<00:01, 1855.10 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 5192/7548 [00:04<00:00, 2887.41 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 5664/7548 [00:04<00:00, 3180.99 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 6135/7548 [00:04<00:00, 2670.48 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 6606/7548 [00:04<00:00, 1639.18 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 7077/7548 [00:05<00:00, 1320.31 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 7548/7548 [00:05<00:00, 1370.67 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 7548/7548 [00:05<00:00, 1279.30 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 5501, 3270, 264, 84183, 263, 10001, 729, 323, 1181, 12159, 13261, 311, 4211, 458, 5675, 429, 282, 4776, 64890, 323, 6422, 13, 19881, 2924, 264, 1273, 729, 323, 1273, 10295, 518, 279, 835, 13, 151645, 198, 151644, 77091, 198, 474, 489, 62437, 198, 474, 489, 62437, 31633, 198, 474, 5708, 198, 474, 19550, 295, 6491, 291, 198, 474, 7834, 198, 474, 489, 62437, 31633, 438, 29796, 198, 1499, 19550, 295, 6491, 291, 1159, 19612, 11, 26036, 1022, 31, 376, 62437, 31055, 354, 2886, 1006, 262, 42309, 5818, 1259, 262, 1376, 33199, 286, 330, 46111, 62, 19, 41513, 62, 16, 756, 286, 330, 39964, 4098, 1245, 756, 286, 330, 39964, 4098, 1604, 756, 286, 330, 46111, 62, 19, 2368, 62, 16, 756, 286, 330, 46111, 62, 16, 41513, 62, 15, 756, 286, 330, 46111, 62, 16, 2368, 62, 15, 756, 286, 330, 46111, 62, 16, 2368, 62, 16, 756, 286, 330, 46111, 62, 19, 2368, 62, 15, 756, 286, 330, 46111, 62, 16, 41513, 62, 16, 756, 286, 330, 46111, 62, 19, 41513, 62, 15, 756, 262, 3211, 340, 31, 376, 62437, 1169, 275, 198, 750, 64890, 60044, 26876, 1006, 262, 15626, 62, 19, 41513, 62, 16, 345, 262, 15626, 62, 16, 21425, 345, 262, 28677, 4098, 1245, 25, 489, 62437, 31633, 36128, 9413, 345, 262, 28677, 4098, 1604, 25, 489, 62437, 31633, 36128, 9413, 345, 262, 15626, 62, 19, 2368, 62, 16, 345, 262, 15626, 62, 16, 41513, 62, 15, 345, 262, 15626, 62, 16, 2368, 62, 15, 345, 262, 15626, 62, 16, 2368, 62, 16, 345, 262, 15626, 62, 19, 2368, 62, 15, 345, 262, 15626, 62, 16, 41513, 62, 16, 345, 262, 15626, 62, 19, 21425, 345, 262, 15626, 62, 19, 41513, 62, 15, 345, 982, 262, 15626, 62, 19, 56924, 62, 15, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 442, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 19, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1245, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1245, 340, 262, 15626, 62, 19, 56924, 62, 16, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 1018, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 19, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1604, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1604, 340, 262, 15626, 62, 19, 6085, 388, 284, 2399, 286, 15626, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 353, 15626, 62, 19, 41513, 62, 15, 198, 286, 488, 15626, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 353, 15626, 62, 19, 41513, 62, 16, 198, 286, 488, 15626, 62, 19, 21425, 198, 262, 1727, 262, 15626, 62, 16, 56924, 62, 15, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 442, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 16, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1245, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1245, 340, 262, 15626, 62, 16, 56924, 62, 16, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 1018, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 16, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1604, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1604, 340, 262, 15626, 62, 16, 6085, 388, 284, 2399, 286, 15626, 62, 16, 56924, 62, 15, 58, 486, 11, 2240, 60, 353, 15626, 62, 16, 41513, 62, 15, 198, 286, 488, 15626, 62, 16, 56924, 62, 16, 58, 4064, 11, 3504, 60, 353, 15626, 62, 16, 41513, 62, 16, 198, 286, 488, 15626, 62, 16, 21425, 198, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 5104, 1006, 310, 15626, 62, 16, 6085, 388, 345, 310, 6911, 4539, 46111, 62, 16, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 16, 2368, 62, 15, 340, 310, 609, 320, 46111, 62, 16, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 16, 2368, 62, 16, 1326, 310, 1008, 5856, 345, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 84058, 1006, 310, 489, 62437, 31633, 5104, 1006, 394, 15626, 62, 19, 6085, 388, 345, 394, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 394, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 394, 1008, 5856, 345, 310, 1727, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 12889, 1006, 310, 489, 62437, 31633, 5104, 1006, 394, 15626, 62, 19, 6085, 388, 345, 394, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 394, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 394, 1008, 5856, 345, 310, 1727, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 25571, 750, 7050, 643, 52111, 60044, 26876, 86025, 62, 16, 11, 15626, 62, 19, 11, 28677, 4098, 1245, 11, 28677, 4098, 1604, 982, 262, 28677, 4098, 1245, 284, 28677, 4098, 1245, 198, 262, 28677, 4098, 1604, 284, 28677, 4098, 1604, 198, 262, 64890, 60044, 26876, 9640, 286, 12459, 8823, 25, 2399, 310, 489, 62437, 520, 611, 86025, 62, 16, 2486, 7, 15, 701, 28677, 4098, 1245, 340, 310, 353, 489, 62437, 520, 611, 86025, 62, 16, 2486, 7, 16, 701, 28677, 4098, 1604, 1326, 286, 1727, 262, 2279, 1006, 286, 15626, 62, 19, 92635, 7, 16, 1326, 286, 15626, 62, 16, 345, 286, 28677, 4098, 1245, 345, 286, 28677, 4098, 1604, 345, 286, 15626, 62, 19, 2486, 7, 16, 1326, 286, 15626, 62, 16, 92635, 7, 15, 1326, 286, 15626, 62, 16, 2486, 7, 15, 1326, 286, 15626, 62, 16, 2486, 7, 16, 1326, 286, 15626, 62, 19, 2486, 7, 15, 1326, 286, 15626, 62, 16, 92635, 7, 16, 1326, 286, 15626, 62, 19, 345, 286, 15626, 62, 19, 92635, 7, 15, 1326, 262, 5125, 39964, 4098, 1245, 284, 19612, 445, 39964, 4098, 1245, 497, 19512, 3618, 340, 39964, 4098, 1604, 284, 19612, 445, 39964, 4098, 1604, 497, 19512, 3618, 3623, 750, 64890, 60044, 5384, 23188, 982, 262, 2550, 13597, 284, 1946, 23188, 2486, 741, 262, 2550, 284, 7834, 9178, 11057, 13597, 11, 3671, 39067, 23188, 18355, 11, 13231, 39067, 23188, 45755, 340, 262, 7050, 643, 52111, 60044, 26876, 1006, 286, 1946, 23188, 11, 2550, 345, 286, 28677, 4098, 1245, 28, 21, 19, 11, 28677, 4098, 1604, 28, 21, 19, 198, 262, 1727, 262, 470, 2550, 271, 750, 1273, 643, 52111, 60044, 5384, 23188, 982, 262, 671, 85658, 37066, 1249, 8767, 291, 33424, 94, 69103, 59151, 198, 262, 19550, 295, 6491, 291, 7645, 284, 64890, 60044, 5384, 23188, 340, 262, 671, 85658, 5355, 51, 21584, 92293, 46451, 104877, 198, 262, 7834, 5287, 284, 1946, 23188, 198, 262, 7834, 5287, 284, 7834, 84058, 36876, 5287, 340, 262, 7834, 5287, 284, 7834, 12889, 36876, 5287, 692, 262, 421, 7834, 7670, 5552, 1445, 29876, 6491, 291, 7645, 11, 7834, 5287, 11, 88655, 28, 16, 68, 12, 20, 982, 286, 1173, 445, 144247, 37066, 1249, 8767, 291, 58143, 5355, 51, 21584, 70568, 106188, 1773, 1138, 262, 770, 510, 286, 1173, 445, 145012, 37066, 1249, 8767, 291, 58143, 5355, 51, 21584, 70568, 16530, 106188, 1773, 1138, 262, 671, 46750, 241, 99540, 59151, 100751, 110760, 198, 262, 1173, 445, 83329, 1249, 8767, 291, 70568, 12147, 19550, 295, 6491, 291, 7645, 340, 262, 1173, 445, 13828, 51, 21584, 70568, 12147, 7834, 5287, 340, 262, 470, 19550, 295, 6491, 291, 7645, 11, 7834, 5287, 271, 333, 1304, 606, 563, 621, 13265, 3817, 21082, 262, 7834, 90223, 33809, 7, 15, 340, 262, 1946, 23188, 284, 7834, 15506, 1188, 16, 21, 11, 220, 16, 21, 701, 3671, 428, 33926, 1138, 262, 1273, 643, 52111, 60044, 5384, 23188, 340, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
Please write a Triton kernel function and its corresponding wrapper to implement an operator that fuses sigmoid and floor. Remember include a test function and test examples at the end.<|im_end|>
<|im_start|>assistant
import triton
import triton.language
import sys
import ninetoothed
import torch
import triton.language as tl
from ninetoothed import Symbol, Tensor



@triton.autotune(
    configs=[],
    key=[
        "tensor_4_stride_1",
        "BLOCK_SIZE_M",
        "BLOCK_SIZE_N",
        "tensor_4_size_1",
        "tensor_1_stride_0",
        "tensor_1_size_0",
        "tensor_1_size_1",
        "tensor_4_size_0",
        "tensor_1_stride_1",
        "tensor_4_stride_0",
    ],
)
@triton.jit
def sigmoid_floor_kernel(
    tensor_4_stride_1,
    tensor_1_pointer,
    BLOCK_SIZE_M: triton.language.constexpr,
    BLOCK_SIZE_N: triton.language.constexpr,
    tensor_4_size_1,
    tensor_1_stride_0,
    tensor_1_size_0,
    tensor_1_size_1,
    tensor_4_size_0,
    tensor_1_stride_1,
    tensor_4_pointer,
    tensor_4_stride_0,
):
    tensor_4_offsets_0 = triton.language.program_id(0) // triton.language.cdiv(
        tensor_4_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_M + triton.language.arange(0, BLOCK_SIZE_M)
    tensor_4_offsets_1 = triton.language.program_id(0) % triton.language.cdiv(
        tensor_4_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_N + triton.language.arange(0, BLOCK_SIZE_N)
    tensor_4_pointers = (
        tensor_4_offsets_0[::, None] * tensor_4_stride_0
        + tensor_4_offsets_1[None, ::] * tensor_4_stride_1
        + tensor_4_pointer
    )
    tensor_1_offsets_0 = triton.language.program_id(0) // triton.language.cdiv(
        tensor_1_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_M + triton.language.arange(0, BLOCK_SIZE_M)
    tensor_1_offsets_1 = triton.language.program_id(0) % triton.language.cdiv(
        tensor_1_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_N + triton.language.arange(0, BLOCK_SIZE_N)
    tensor_1_pointers = (
        tensor_1_offsets_0[::, None] * tensor_1_stride_0
        + tensor_1_offsets_1[None, ::] * tensor_1_stride_1
        + tensor_1_pointer
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.load(
            tensor_1_pointers,
            mask=(tensor_1_offsets_0[::, None] < tensor_1_size_0)
            & (tensor_1_offsets_1[None, ::] < tensor_1_size_1),
            other=None,
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.sigmoid(
            triton.language.load(
                tensor_4_pointers,
                mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
                & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
                other=None,
            )
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.floor(
            triton.language.load(
                tensor_4_pointers,
                mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
                & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
                other=None,
            )
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )


def launch_sigmoid_floor_kernel(tensor_1, tensor_4, BLOCK_SIZE_M, BLOCK_SIZE_N):
    BLOCK_SIZE_M = BLOCK_SIZE_M
    BLOCK_SIZE_N = BLOCK_SIZE_N
    sigmoid_floor_kernel[
        lambda meta: (
            triton.cdiv(tensor_1.size(0), BLOCK_SIZE_M)
            * triton.cdiv(tensor_1.size(1), BLOCK_SIZE_N),
        )
    ](
        tensor_4.stride(1),
        tensor_1,
        BLOCK_SIZE_M,
        BLOCK_SIZE_N,
        tensor_4.size(1),
        tensor_1.stride(0),
        tensor_1.size(0),
        tensor_1.size(1),
        tensor_4.size(0),
        tensor_1.stride(1),
        tensor_4,
        tensor_4.stride(0),
    )

BLOCK_SIZE_M = Symbol("BLOCK_SIZE_M", constexpr=True)
BLOCK_SIZE_N = Symbol("BLOCK_SIZE_N", constexpr=True)


def sigmoid_floor(input_tensor):
    output_shape = input_tensor.size()
    output = torch.empty(output_shape, device=input_tensor.device, dtype=input_tensor.dtype)
    launch_sigmoid_floor_kernel(
        input_tensor, output,
        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64
    )
    return output

def test_sigmoid_floor(input_tensor):
    # 使用 NineToothed 计算结果
    ninetoothed_output = sigmoid_floor(input_tensor)
    # 使用 PyTorch 实现对比
    torch_result = input_tensor
    torch_result = torch.sigmoid(torch_result)
    torch_result = torch.floor(torch_result)

    if torch.allclose(ninetoothed_output, torch_result, atol=1e-5):
        print("✅ NineToothed 和 PyTorch 输出匹配。")
    else:
        print("❌ NineToothed 和 PyTorch 输出不匹配。")
    # 打印结果用于调试
    print("NineToothed 输出:", ninetoothed_output)
    print("PyTorch 输出:", torch_result)
    return ninetoothed_output, torch_result

if __name__ == "__main__":
    torch.manual_seed(0)
    input_tensor = torch.rand((16, 16), device="cuda")
    test_sigmoid_floor(input_tensor)
<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 474, 489, 62437, 198, 474, 489, 62437, 31633, 198, 474, 5708, 198, 474, 19550, 295, 6491, 291, 198, 474, 7834, 198, 474, 489, 62437, 31633, 438, 29796, 198, 1499, 19550, 295, 6491, 291, 1159, 19612, 11, 26036, 1022, 31, 376, 62437, 31055, 354, 2886, 1006, 262, 42309, 5818, 1259, 262, 1376, 33199, 286, 330, 46111, 62, 19, 41513, 62, 16, 756, 286, 330, 39964, 4098, 1245, 756, 286, 330, 39964, 4098, 1604, 756, 286, 330, 46111, 62, 19, 2368, 62, 16, 756, 286, 330, 46111, 62, 16, 41513, 62, 15, 756, 286, 330, 46111, 62, 16, 2368, 62, 15, 756, 286, 330, 46111, 62, 16, 2368, 62, 16, 756, 286, 330, 46111, 62, 19, 2368, 62, 15, 756, 286, 330, 46111, 62, 16, 41513, 62, 16, 756, 286, 330, 46111, 62, 19, 41513, 62, 15, 756, 262, 3211, 340, 31, 376, 62437, 1169, 275, 198, 750, 64890, 60044, 26876, 1006, 262, 15626, 62, 19, 41513, 62, 16, 345, 262, 15626, 62, 16, 21425, 345, 262, 28677, 4098, 1245, 25, 489, 62437, 31633, 36128, 9413, 345, 262, 28677, 4098, 1604, 25, 489, 62437, 31633, 36128, 9413, 345, 262, 15626, 62, 19, 2368, 62, 16, 345, 262, 15626, 62, 16, 41513, 62, 15, 345, 262, 15626, 62, 16, 2368, 62, 15, 345, 262, 15626, 62, 16, 2368, 62, 16, 345, 262, 15626, 62, 19, 2368, 62, 15, 345, 262, 15626, 62, 16, 41513, 62, 16, 345, 262, 15626, 62, 19, 21425, 345, 262, 15626, 62, 19, 41513, 62, 15, 345, 982, 262, 15626, 62, 19, 56924, 62, 15, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 442, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 19, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1245, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1245, 340, 262, 15626, 62, 19, 56924, 62, 16, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 1018, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 19, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1604, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1604, 340, 262, 15626, 62, 19, 6085, 388, 284, 2399, 286, 15626, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 353, 15626, 62, 19, 41513, 62, 15, 198, 286, 488, 15626, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 353, 15626, 62, 19, 41513, 62, 16, 198, 286, 488, 15626, 62, 19, 21425, 198, 262, 1727, 262, 15626, 62, 16, 56924, 62, 15, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 442, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 16, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1245, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1245, 340, 262, 15626, 62, 16, 56924, 62, 16, 284, 489, 62437, 31633, 45082, 842, 7, 15, 8, 1018, 489, 62437, 31633, 520, 611, 1006, 286, 15626, 62, 16, 2368, 62, 16, 11, 28677, 4098, 1604, 198, 262, 873, 353, 28677, 4098, 1604, 488, 489, 62437, 31633, 24315, 7, 15, 11, 28677, 4098, 1604, 340, 262, 15626, 62, 16, 6085, 388, 284, 2399, 286, 15626, 62, 16, 56924, 62, 15, 58, 486, 11, 2240, 60, 353, 15626, 62, 16, 41513, 62, 15, 198, 286, 488, 15626, 62, 16, 56924, 62, 16, 58, 4064, 11, 3504, 60, 353, 15626, 62, 16, 41513, 62, 16, 198, 286, 488, 15626, 62, 16, 21425, 198, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 5104, 1006, 310, 15626, 62, 16, 6085, 388, 345, 310, 6911, 4539, 46111, 62, 16, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 16, 2368, 62, 15, 340, 310, 609, 320, 46111, 62, 16, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 16, 2368, 62, 16, 1326, 310, 1008, 5856, 345, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 84058, 1006, 310, 489, 62437, 31633, 5104, 1006, 394, 15626, 62, 19, 6085, 388, 345, 394, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 394, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 394, 1008, 5856, 345, 310, 1727, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 1727, 262, 489, 62437, 31633, 16114, 1006, 286, 15626, 62, 19, 6085, 388, 345, 286, 489, 62437, 31633, 12889, 1006, 310, 489, 62437, 31633, 5104, 1006, 394, 15626, 62, 19, 6085, 388, 345, 394, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 394, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 394, 1008, 5856, 345, 310, 1727, 286, 2837, 286, 6911, 4539, 46111, 62, 19, 56924, 62, 15, 58, 486, 11, 2240, 60, 366, 15626, 62, 19, 2368, 62, 15, 340, 286, 609, 320, 46111, 62, 19, 56924, 62, 16, 58, 4064, 11, 3504, 60, 366, 15626, 62, 19, 2368, 62, 16, 1326, 262, 25571, 750, 7050, 643, 52111, 60044, 26876, 86025, 62, 16, 11, 15626, 62, 19, 11, 28677, 4098, 1245, 11, 28677, 4098, 1604, 982, 262, 28677, 4098, 1245, 284, 28677, 4098, 1245, 198, 262, 28677, 4098, 1604, 284, 28677, 4098, 1604, 198, 262, 64890, 60044, 26876, 9640, 286, 12459, 8823, 25, 2399, 310, 489, 62437, 520, 611, 86025, 62, 16, 2486, 7, 15, 701, 28677, 4098, 1245, 340, 310, 353, 489, 62437, 520, 611, 86025, 62, 16, 2486, 7, 16, 701, 28677, 4098, 1604, 1326, 286, 1727, 262, 2279, 1006, 286, 15626, 62, 19, 92635, 7, 16, 1326, 286, 15626, 62, 16, 345, 286, 28677, 4098, 1245, 345, 286, 28677, 4098, 1604, 345, 286, 15626, 62, 19, 2486, 7, 16, 1326, 286, 15626, 62, 16, 92635, 7, 15, 1326, 286, 15626, 62, 16, 2486, 7, 15, 1326, 286, 15626, 62, 16, 2486, 7, 16, 1326, 286, 15626, 62, 19, 2486, 7, 15, 1326, 286, 15626, 62, 16, 92635, 7, 16, 1326, 286, 15626, 62, 19, 345, 286, 15626, 62, 19, 92635, 7, 15, 1326, 262, 5125, 39964, 4098, 1245, 284, 19612, 445, 39964, 4098, 1245, 497, 19512, 3618, 340, 39964, 4098, 1604, 284, 19612, 445, 39964, 4098, 1604, 497, 19512, 3618, 3623, 750, 64890, 60044, 5384, 23188, 982, 262, 2550, 13597, 284, 1946, 23188, 2486, 741, 262, 2550, 284, 7834, 9178, 11057, 13597, 11, 3671, 39067, 23188, 18355, 11, 13231, 39067, 23188, 45755, 340, 262, 7050, 643, 52111, 60044, 26876, 1006, 286, 1946, 23188, 11, 2550, 345, 286, 28677, 4098, 1245, 28, 21, 19, 11, 28677, 4098, 1604, 28, 21, 19, 198, 262, 1727, 262, 470, 2550, 271, 750, 1273, 643, 52111, 60044, 5384, 23188, 982, 262, 671, 85658, 37066, 1249, 8767, 291, 33424, 94, 69103, 59151, 198, 262, 19550, 295, 6491, 291, 7645, 284, 64890, 60044, 5384, 23188, 340, 262, 671, 85658, 5355, 51, 21584, 92293, 46451, 104877, 198, 262, 7834, 5287, 284, 1946, 23188, 198, 262, 7834, 5287, 284, 7834, 84058, 36876, 5287, 340, 262, 7834, 5287, 284, 7834, 12889, 36876, 5287, 692, 262, 421, 7834, 7670, 5552, 1445, 29876, 6491, 291, 7645, 11, 7834, 5287, 11, 88655, 28, 16, 68, 12, 20, 982, 286, 1173, 445, 144247, 37066, 1249, 8767, 291, 58143, 5355, 51, 21584, 70568, 106188, 1773, 1138, 262, 770, 510, 286, 1173, 445, 145012, 37066, 1249, 8767, 291, 58143, 5355, 51, 21584, 70568, 16530, 106188, 1773, 1138, 262, 671, 46750, 241, 99540, 59151, 100751, 110760, 198, 262, 1173, 445, 83329, 1249, 8767, 291, 70568, 12147, 19550, 295, 6491, 291, 7645, 340, 262, 1173, 445, 13828, 51, 21584, 70568, 12147, 7834, 5287, 340, 262, 470, 19550, 295, 6491, 291, 7645, 11, 7834, 5287, 271, 333, 1304, 606, 563, 621, 13265, 3817, 21082, 262, 7834, 90223, 33809, 7, 15, 340, 262, 1946, 23188, 284, 7834, 15506, 1188, 16, 21, 11, 220, 16, 21, 701, 3671, 428, 33926, 1138, 262, 1273, 643, 52111, 60044, 5384, 23188, 340, 151645]
labels:
import triton
import triton.language
import sys
import ninetoothed
import torch
import triton.language as tl
from ninetoothed import Symbol, Tensor



@triton.autotune(
    configs=[],
    key=[
        "tensor_4_stride_1",
        "BLOCK_SIZE_M",
        "BLOCK_SIZE_N",
        "tensor_4_size_1",
        "tensor_1_stride_0",
        "tensor_1_size_0",
        "tensor_1_size_1",
        "tensor_4_size_0",
        "tensor_1_stride_1",
        "tensor_4_stride_0",
    ],
)
@triton.jit
def sigmoid_floor_kernel(
    tensor_4_stride_1,
    tensor_1_pointer,
    BLOCK_SIZE_M: triton.language.constexpr,
    BLOCK_SIZE_N: triton.language.constexpr,
    tensor_4_size_1,
    tensor_1_stride_0,
    tensor_1_size_0,
    tensor_1_size_1,
    tensor_4_size_0,
    tensor_1_stride_1,
    tensor_4_pointer,
    tensor_4_stride_0,
):
    tensor_4_offsets_0 = triton.language.program_id(0) // triton.language.cdiv(
        tensor_4_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_M + triton.language.arange(0, BLOCK_SIZE_M)
    tensor_4_offsets_1 = triton.language.program_id(0) % triton.language.cdiv(
        tensor_4_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_N + triton.language.arange(0, BLOCK_SIZE_N)
    tensor_4_pointers = (
        tensor_4_offsets_0[::, None] * tensor_4_stride_0
        + tensor_4_offsets_1[None, ::] * tensor_4_stride_1
        + tensor_4_pointer
    )
    tensor_1_offsets_0 = triton.language.program_id(0) // triton.language.cdiv(
        tensor_1_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_M + triton.language.arange(0, BLOCK_SIZE_M)
    tensor_1_offsets_1 = triton.language.program_id(0) % triton.language.cdiv(
        tensor_1_size_1, BLOCK_SIZE_N
    ) * BLOCK_SIZE_N + triton.language.arange(0, BLOCK_SIZE_N)
    tensor_1_pointers = (
        tensor_1_offsets_0[::, None] * tensor_1_stride_0
        + tensor_1_offsets_1[None, ::] * tensor_1_stride_1
        + tensor_1_pointer
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.load(
            tensor_1_pointers,
            mask=(tensor_1_offsets_0[::, None] < tensor_1_size_0)
            & (tensor_1_offsets_1[None, ::] < tensor_1_size_1),
            other=None,
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.sigmoid(
            triton.language.load(
                tensor_4_pointers,
                mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
                & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
                other=None,
            )
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )
    triton.language.store(
        tensor_4_pointers,
        triton.language.floor(
            triton.language.load(
                tensor_4_pointers,
                mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
                & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
                other=None,
            )
        ),
        mask=(tensor_4_offsets_0[::, None] < tensor_4_size_0)
        & (tensor_4_offsets_1[None, ::] < tensor_4_size_1),
    )


def launch_sigmoid_floor_kernel(tensor_1, tensor_4, BLOCK_SIZE_M, BLOCK_SIZE_N):
    BLOCK_SIZE_M = BLOCK_SIZE_M
    BLOCK_SIZE_N = BLOCK_SIZE_N
    sigmoid_floor_kernel[
        lambda meta: (
            triton.cdiv(tensor_1.size(0), BLOCK_SIZE_M)
            * triton.cdiv(tensor_1.size(1), BLOCK_SIZE_N),
        )
    ](
        tensor_4.stride(1),
        tensor_1,
        BLOCK_SIZE_M,
        BLOCK_SIZE_N,
        tensor_4.size(1),
        tensor_1.stride(0),
        tensor_1.size(0),
        tensor_1.size(1),
        tensor_4.size(0),
        tensor_1.stride(1),
        tensor_4,
        tensor_4.stride(0),
    )

BLOCK_SIZE_M = Symbol("BLOCK_SIZE_M", constexpr=True)
BLOCK_SIZE_N = Symbol("BLOCK_SIZE_N", constexpr=True)


def sigmoid_floor(input_tensor):
    output_shape = input_tensor.size()
    output = torch.empty(output_shape, device=input_tensor.device, dtype=input_tensor.dtype)
    launch_sigmoid_floor_kernel(
        input_tensor, output,
        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64
    )
    return output

def test_sigmoid_floor(input_tensor):
    # 使用 NineToothed 计算结果
    ninetoothed_output = sigmoid_floor(input_tensor)
    # 使用 PyTorch 实现对比
    torch_result = input_tensor
    torch_result = torch.sigmoid(torch_result)
    torch_result = torch.floor(torch_result)

    if torch.allclose(ninetoothed_output, torch_result, atol=1e-5):
        print("✅ NineToothed 和 PyTorch 输出匹配。")
    else:
        print("❌ NineToothed 和 PyTorch 输出不匹配。")
    # 打印结果用于调试
    print("NineToothed 输出:", ninetoothed_output)
    print("PyTorch 输出:", torch_result)
    return ninetoothed_output, torch_result

if __name__ == "__main__":
    torch.manual_seed(0)
    input_tensor = torch.rand((16, 16), device="cuda")
    test_sigmoid_floor(input_tensor)
<|im_end|>
[INFO|configuration_utils.py:677] 2024-11-27 13:37:19,902 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/config.json
[INFO|configuration_utils.py:746] 2024-11-27 13:37:19,903 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3934] 2024-11-27 13:37:21,610 >> loading weights file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2024-11-27 13:37:21,617 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2024-11-27 13:37:21,619 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:27<01:22, 27.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:53, 26.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.88s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.93s/it]
[INFO|modeling_utils.py:4800] 2024-11-27 13:38:45,465 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2024-11-27 13:38:45,466 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.92s/it]
[INFO|configuration_utils.py:1049] 2024-11-27 13:38:45,473 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2024-11-27 13:38:45,473 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.93s/it]
[INFO|2024-11-27 13:38:45] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2024-11-27 13:38:45] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2024-11-27 13:38:45] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2024-11-27 13:38:45] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2024-11-27 13:38:45] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,gate_proj,o_proj,k_proj,up_proj,v_proj,down_proj
Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 17.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:23<00:00, 20.94s/it]
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
[INFO|2024-11-27 13:38:45] llamafactory.model.loader:157 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/data/home/ljl/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
[INFO|trainer.py:698] 2024-11-27 13:38:45,837 >> Using auto half precision backend
[INFO|trainer.py:2313] 2024-11-27 13:38:46,257 >> ***** Running training *****
[INFO|trainer.py:2314] 2024-11-27 13:38:46,257 >>   Num examples = 6,793
[INFO|trainer.py:2315] 2024-11-27 13:38:46,258 >>   Num Epochs = 3
[INFO|trainer.py:2316] 2024-11-27 13:38:46,258 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2319] 2024-11-27 13:38:46,258 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2320] 2024-11-27 13:38:46,258 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2321] 2024-11-27 13:38:46,258 >>   Total optimization steps = 318
[INFO|trainer.py:2322] 2024-11-27 13:38:46,261 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:10<55:58, 10.60s/it]  1%|          | 2/318 [00:22<1:00:41, 11.52s/it]  1%|          | 3/318 [00:33<58:53, 11.22s/it]    1%|▏         | 4/318 [00:46<1:01:40, 11.79s/it]  2%|▏         | 5/318 [00:57<1:00:01, 11.51s/it]  2%|▏         | 6/318 [01:07<57:23, 11.04s/it]    2%|▏         | 7/318 [01:18<56:53, 10.97s/it]  3%|▎         | 8/318 [01:29<57:05, 11.05s/it]  3%|▎         | 9/318 [01:40<56:53, 11.05s/it]  3%|▎         | 10/318 [01:51<56:00, 10.91s/it]                                                {'loss': 0.421, 'grad_norm': 0.0779457539319992, 'learning_rate': 3.125e-06, 'epoch': 0.09}
  3%|▎         | 10/318 [01:51<56:00, 10.91s/it]  3%|▎         | 11/318 [01:59<51:56, 10.15s/it]  4%|▍         | 12/318 [02:09<51:44, 10.15s/it]  4%|▍         | 13/318 [02:18<50:16,  9.89s/it]  4%|▍         | 14/318 [02:31<53:54, 10.64s/it]  5%|▍         | 15/318 [02:42<54:12, 10.73s/it]  5%|▌         | 16/318 [02:54<55:50, 11.09s/it]  5%|▌         | 17/318 [03:05<55:27, 11.06s/it]  6%|▌         | 18/318 [03:16<55:55, 11.19s/it]  6%|▌         | 19/318 [03:26<53:25, 10.72s/it]  6%|▋         | 20/318 [03:37<54:08, 10.90s/it]                                                {'loss': 0.4254, 'grad_norm': 0.06532809883356094, 'learning_rate': 6.25e-06, 'epoch': 0.19}
  6%|▋         | 20/318 [03:37<54:08, 10.90s/it]  7%|▋         | 21/318 [03:47<52:23, 10.59s/it]  7%|▋         | 22/318 [03:57<51:39, 10.47s/it]  7%|▋         | 23/318 [04:08<51:18, 10.44s/it]  8%|▊         | 24/318 [04:15<46:59,  9.59s/it]  8%|▊         | 25/318 [04:26<47:57,  9.82s/it]  8%|▊         | 26/318 [04:35<47:17,  9.72s/it]  8%|▊         | 27/318 [04:45<47:25,  9.78s/it]  9%|▉         | 28/318 [04:57<50:14, 10.40s/it]  9%|▉         | 29/318 [05:07<49:44, 10.33s/it]  9%|▉         | 30/318 [05:18<50:02, 10.43s/it]                                                {'loss': 0.4257, 'grad_norm': 0.09850770980119705, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.28}
  9%|▉         | 30/318 [05:18<50:02, 10.43s/it] 10%|▉         | 31/318 [05:26<46:50,  9.79s/it] 10%|█         | 32/318 [05:36<46:50,  9.83s/it] 10%|█         | 33/318 [05:48<50:18, 10.59s/it] 11%|█         | 34/318 [06:00<51:24, 10.86s/it] 11%|█         | 35/318 [06:12<53:42, 11.39s/it] 11%|█▏        | 36/318 [06:25<55:20, 11.78s/it] 12%|█▏        | 37/318 [06:35<52:43, 11.26s/it] 12%|█▏        | 38/318 [06:45<50:17, 10.78s/it] 12%|█▏        | 39/318 [06:56<50:24, 10.84s/it] 13%|█▎        | 40/318 [07:07<50:45, 10.95s/it]                                                {'loss': 0.4239, 'grad_norm': 0.11013876646757126, 'learning_rate': 9.980706626858607e-06, 'epoch': 0.38}
 13%|█▎        | 40/318 [07:07<50:45, 10.95s/it] 13%|█▎        | 41/318 [07:17<49:43, 10.77s/it] 13%|█▎        | 42/318 [07:26<46:14, 10.05s/it] 14%|█▎        | 43/318 [07:36<46:22, 10.12s/it] 14%|█▍        | 44/318 [07:48<48:57, 10.72s/it] 14%|█▍        | 45/318 [07:59<49:40, 10.92s/it] 14%|█▍        | 46/318 [08:09<48:04, 10.61s/it] 15%|█▍        | 47/318 [08:22<50:37, 11.21s/it] 15%|█▌        | 48/318 [08:32<48:44, 10.83s/it] 15%|█▌        | 49/318 [08:42<47:41, 10.64s/it] 16%|█▌        | 50/318 [08:54<49:16, 11.03s/it]                                                {'loss': 0.3994, 'grad_norm': 0.10058827698230743, 'learning_rate': 9.90258241271112e-06, 'epoch': 0.47}
 16%|█▌        | 50/318 [08:54<49:16, 11.03s/it] 16%|█▌        | 51/318 [09:04<47:40, 10.71s/it] 16%|█▋        | 52/318 [09:15<47:53, 10.80s/it] 17%|█▋        | 53/318 [09:25<47:05, 10.66s/it] 17%|█▋        | 54/318 [09:38<49:27, 11.24s/it] 17%|█▋        | 55/318 [09:49<49:34, 11.31s/it] 18%|█▊        | 56/318 [10:00<47:53, 10.97s/it] 18%|█▊        | 57/318 [10:10<47:10, 10.84s/it] 18%|█▊        | 58/318 [10:23<49:11, 11.35s/it] 19%|█▊        | 59/318 [10:33<47:47, 11.07s/it] 19%|█▉        | 60/318 [10:43<46:22, 10.79s/it]                                                {'loss': 0.3751, 'grad_norm': 0.10502507537603378, 'learning_rate': 9.765362502737098e-06, 'epoch': 0.56}
 19%|█▉        | 60/318 [10:43<46:22, 10.79s/it] 19%|█▉        | 61/318 [10:54<46:25, 10.84s/it] 19%|█▉        | 62/318 [11:04<45:02, 10.56s/it] 20%|█▉        | 63/318 [11:17<47:20, 11.14s/it] 20%|██        | 64/318 [11:26<44:57, 10.62s/it] 20%|██        | 65/318 [11:36<44:16, 10.50s/it] 21%|██        | 66/318 [11:47<44:56, 10.70s/it] 21%|██        | 67/318 [11:57<43:55, 10.50s/it] 21%|██▏       | 68/318 [12:08<44:23, 10.65s/it] 22%|██▏       | 69/318 [12:18<43:28, 10.48s/it] 22%|██▏       | 70/318 [12:28<41:46, 10.11s/it]                                                {'loss': 0.3551, 'grad_norm': 0.08696430176496506, 'learning_rate': 9.570700944819584e-06, 'epoch': 0.66}
 22%|██▏       | 70/318 [12:28<41:46, 10.11s/it] 22%|██▏       | 71/318 [12:37<40:40,  9.88s/it] 23%|██▎       | 72/318 [12:49<42:33, 10.38s/it] 23%|██▎       | 73/318 [13:01<44:30, 10.90s/it] 23%|██▎       | 74/318 [13:12<44:26, 10.93s/it] 24%|██▎       | 75/318 [13:22<44:00, 10.87s/it] 24%|██▍       | 76/318 [13:33<43:57, 10.90s/it] 24%|██▍       | 77/318 [13:43<42:07, 10.49s/it] 25%|██▍       | 78/318 [13:56<44:32, 11.14s/it] 25%|██▍       | 79/318 [14:05<41:53, 10.52s/it] 25%|██▌       | 80/318 [14:15<41:07, 10.37s/it]                                                {'loss': 0.3176, 'grad_norm': 0.08698659390211105, 'learning_rate': 9.320944188084241e-06, 'epoch': 0.75}
 25%|██▌       | 80/318 [14:15<41:07, 10.37s/it] 25%|██▌       | 81/318 [14:25<40:24, 10.23s/it] 26%|██▌       | 82/318 [14:35<40:09, 10.21s/it] 26%|██▌       | 83/318 [14:47<42:52, 10.95s/it] 26%|██▋       | 84/318 [14:58<41:53, 10.74s/it] 27%|██▋       | 85/318 [15:07<40:41, 10.48s/it] 27%|██▋       | 86/318 [15:18<40:32, 10.49s/it] 27%|██▋       | 87/318 [15:31<43:00, 11.17s/it] 28%|██▊       | 88/318 [15:41<42:02, 10.97s/it] 28%|██▊       | 89/318 [15:53<43:11, 11.32s/it] 28%|██▊       | 90/318 [16:03<41:35, 10.94s/it]                                                {'loss': 0.3144, 'grad_norm': 0.09042298048734665, 'learning_rate': 9.019102798817196e-06, 'epoch': 0.85}
 28%|██▊       | 90/318 [16:03<41:35, 10.94s/it] 29%|██▊       | 91/318 [16:14<41:13, 10.90s/it] 29%|██▉       | 92/318 [16:25<40:39, 10.80s/it] 29%|██▉       | 93/318 [16:37<42:33, 11.35s/it] 30%|██▉       | 94/318 [16:47<40:53, 10.95s/it] 30%|██▉       | 95/318 [17:00<42:06, 11.33s/it] 30%|███       | 96/318 [17:12<43:26, 11.74s/it] 31%|███       | 97/318 [17:24<42:50, 11.63s/it] 31%|███       | 98/318 [17:34<40:48, 11.13s/it] 31%|███       | 99/318 [17:44<39:28, 10.82s/it] 31%|███▏      | 100/318 [17:54<39:01, 10.74s/it]                                                 {'loss': 0.3101, 'grad_norm': 0.07424952834844589, 'learning_rate': 8.66881517111902e-06, 'epoch': 0.94}
 31%|███▏      | 100/318 [17:54<39:01, 10.74s/it] 32%|███▏      | 101/318 [18:06<39:23, 10.89s/it] 32%|███▏      | 102/318 [18:16<38:17, 10.64s/it] 32%|███▏      | 103/318 [18:28<40:14, 11.23s/it] 33%|███▎      | 104/318 [18:41<41:35, 11.66s/it] 33%|███▎      | 105/318 [18:53<42:06, 11.86s/it] 33%|███▎      | 106/318 [19:04<40:34, 11.48s/it] 34%|███▎      | 107/318 [19:13<38:22, 10.91s/it] 34%|███▍      | 108/318 [19:26<39:52, 11.39s/it] 34%|███▍      | 109/318 [19:37<39:08, 11.24s/it] 35%|███▍      | 110/318 [19:45<36:14, 10.46s/it]                                                 {'loss': 0.2988, 'grad_norm': 0.07471266388893127, 'learning_rate': 8.274303669726427e-06, 'epoch': 1.03}
 35%|███▍      | 110/318 [19:45<36:14, 10.46s/it] 35%|███▍      | 111/318 [19:58<38:11, 11.07s/it] 35%|███▌      | 112/318 [20:08<37:05, 10.81s/it] 36%|███▌      | 113/318 [20:17<34:42, 10.16s/it] 36%|███▌      | 114/318 [20:27<34:09, 10.05s/it] 36%|███▌      | 115/318 [20:36<33:47,  9.99s/it] 36%|███▋      | 116/318 [20:47<34:31, 10.25s/it] 37%|███▋      | 117/318 [20:58<34:21, 10.26s/it] 37%|███▋      | 118/318 [21:08<34:26, 10.33s/it] 37%|███▋      | 119/318 [21:21<36:27, 10.99s/it] 38%|███▊      | 120/318 [21:33<37:53, 11.48s/it]                                                 {'loss': 0.2729, 'grad_norm': 0.0800897479057312, 'learning_rate': 7.84032373365578e-06, 'epoch': 1.13}
 38%|███▊      | 120/318 [21:33<37:53, 11.48s/it] 38%|███▊      | 121/318 [21:43<36:07, 11.00s/it] 38%|███▊      | 122/318 [21:54<35:55, 11.00s/it] 39%|███▊      | 123/318 [22:07<37:19, 11.48s/it] 39%|███▉      | 124/318 [22:17<36:24, 11.26s/it] 39%|███▉      | 125/318 [22:29<36:39, 11.40s/it] 40%|███▉      | 126/318 [22:39<34:42, 10.85s/it] 40%|███▉      | 127/318 [22:49<34:14, 10.76s/it] 40%|████      | 128/318 [22:59<32:57, 10.41s/it] 41%|████      | 129/318 [23:09<32:29, 10.31s/it] 41%|████      | 130/318 [23:22<34:34, 11.04s/it]                                                 {'loss': 0.2751, 'grad_norm': 0.07444725185632706, 'learning_rate': 7.372106554172802e-06, 'epoch': 1.22}
 41%|████      | 130/318 [23:22<34:34, 11.04s/it] 41%|████      | 131/318 [23:33<34:27, 11.06s/it] 42%|████▏     | 132/318 [23:46<35:47, 11.55s/it] 42%|████▏     | 133/318 [23:57<35:11, 11.42s/it] 42%|████▏     | 134/318 [24:09<36:09, 11.79s/it] 42%|████▏     | 135/318 [24:19<34:23, 11.28s/it] 43%|████▎     | 136/318 [24:30<33:23, 11.01s/it] 43%|████▎     | 137/318 [24:40<32:57, 10.93s/it] 43%|████▎     | 138/318 [24:51<32:13, 10.74s/it] 44%|████▎     | 139/318 [25:01<31:17, 10.49s/it] 44%|████▍     | 140/318 [25:12<31:25, 10.60s/it]                                                 {'loss': 0.264, 'grad_norm': 0.08026066422462463, 'learning_rate': 6.87529601804781e-06, 'epoch': 1.31}
 44%|████▍     | 140/318 [25:12<31:25, 10.60s/it] 44%|████▍     | 141/318 [25:22<30:49, 10.45s/it] 45%|████▍     | 142/318 [25:34<32:28, 11.07s/it] 45%|████▍     | 143/318 [25:45<32:13, 11.05s/it] 45%|████▌     | 144/318 [25:56<31:32, 10.88s/it] 46%|████▌     | 145/318 [26:06<30:45, 10.66s/it] 46%|████▌     | 146/318 [26:18<32:16, 11.26s/it] 46%|████▌     | 147/318 [26:28<30:29, 10.70s/it] 47%|████▋     | 148/318 [26:38<29:56, 10.57s/it] 47%|████▋     | 149/318 [26:49<29:37, 10.52s/it] 47%|████▋     | 150/318 [27:00<29:58, 10.71s/it]                                                 {'loss': 0.243, 'grad_norm': 0.07191531360149384, 'learning_rate': 6.355880676182086e-06, 'epoch': 1.41}
 47%|████▋     | 150/318 [27:00<29:58, 10.71s/it] 47%|████▋     | 151/318 [27:10<29:24, 10.57s/it] 48%|████▊     | 152/318 [27:20<28:51, 10.43s/it] 48%|████▊     | 153/318 [27:30<27:58, 10.17s/it] 48%|████▊     | 154/318 [27:38<26:17,  9.62s/it] 49%|████▊     | 155/318 [27:49<27:26, 10.10s/it] 49%|████▉     | 156/318 [28:00<28:07, 10.42s/it] 49%|████▉     | 157/318 [28:10<27:45, 10.34s/it] 50%|████▉     | 158/318 [28:22<28:49, 10.81s/it] 50%|█████     | 159/318 [28:35<30:10, 11.39s/it] 50%|█████     | 160/318 [28:46<29:22, 11.15s/it]                                                 {'loss': 0.2149, 'grad_norm': 0.07493176311254501, 'learning_rate': 5.820121557655109e-06, 'epoch': 1.5}
 50%|█████     | 160/318 [28:46<29:22, 11.15s/it] 51%|█████     | 161/318 [28:57<29:19, 11.21s/it] 51%|█████     | 162/318 [29:08<29:03, 11.18s/it] 51%|█████▏    | 163/318 [29:19<28:40, 11.10s/it] 52%|█████▏    | 164/318 [29:28<26:41, 10.40s/it] 52%|█████▏    | 165/318 [29:38<26:35, 10.43s/it] 52%|█████▏    | 166/318 [29:51<27:55, 11.02s/it] 53%|█████▎    | 167/318 [30:03<28:55, 11.49s/it] 53%|█████▎    | 168/318 [30:15<29:14, 11.70s/it] 53%|█████▎    | 169/318 [30:26<27:49, 11.20s/it] 53%|█████▎    | 170/318 [30:36<27:17, 11.07s/it]                                                 {'loss': 0.2094, 'grad_norm': 0.0793098509311676, 'learning_rate': 5.274476699321638e-06, 'epoch': 1.6}
 53%|█████▎    | 170/318 [30:36<27:17, 11.07s/it] 54%|█████▍    | 171/318 [30:48<27:30, 11.23s/it] 54%|█████▍    | 172/318 [30:56<24:59, 10.27s/it] 54%|█████▍    | 173/318 [31:05<24:10, 10.00s/it] 55%|█████▍    | 174/318 [31:15<23:59, 10.00s/it] 55%|█████▌    | 175/318 [31:25<23:22,  9.81s/it] 55%|█████▌    | 176/318 [31:35<23:21,  9.87s/it] 56%|█████▌    | 177/318 [31:45<23:14,  9.89s/it] 56%|█████▌    | 178/318 [31:56<23:47, 10.20s/it] 56%|█████▋    | 179/318 [32:08<25:14, 10.90s/it] 57%|█████▋    | 180/318 [32:19<25:07, 10.93s/it]                                                 {'loss': 0.2138, 'grad_norm': 0.06751546263694763, 'learning_rate': 4.7255233006783626e-06, 'epoch': 1.69}
 57%|█████▋    | 180/318 [32:19<25:07, 10.93s/it] 57%|█████▋    | 181/318 [32:31<25:20, 11.10s/it] 57%|█████▋    | 182/318 [32:40<24:02, 10.61s/it] 58%|█████▊    | 183/318 [32:49<22:40, 10.08s/it] 58%|█████▊    | 184/318 [32:59<22:36, 10.13s/it] 58%|█████▊    | 185/318 [33:09<22:23, 10.11s/it] 58%|█████▊    | 186/318 [33:19<22:06, 10.05s/it] 59%|█████▉    | 187/318 [33:30<22:13, 10.18s/it] 59%|█████▉    | 188/318 [33:40<22:03, 10.18s/it] 59%|█████▉    | 189/318 [33:50<21:46, 10.13s/it] 60%|█████▉    | 190/318 [33:59<21:21, 10.01s/it]                                                 {'loss': 0.1954, 'grad_norm': 0.0674436017870903, 'learning_rate': 4.179878442344892e-06, 'epoch': 1.78}
 60%|█████▉    | 190/318 [33:59<21:21, 10.01s/it] 60%|██████    | 191/318 [34:09<21:07,  9.98s/it] 60%|██████    | 192/318 [34:20<21:21, 10.17s/it] 61%|██████    | 193/318 [34:31<21:29, 10.32s/it] 61%|██████    | 194/318 [34:43<22:46, 11.02s/it] 61%|██████▏   | 195/318 [34:53<21:57, 10.71s/it] 62%|██████▏   | 196/318 [35:06<22:54, 11.26s/it] 62%|██████▏   | 197/318 [35:15<21:40, 10.75s/it] 62%|██████▏   | 198/318 [35:28<22:36, 11.30s/it] 63%|██████▎   | 199/318 [35:37<21:15, 10.72s/it] 63%|██████▎   | 200/318 [35:48<21:01, 10.69s/it]                                                 {'loss': 0.1916, 'grad_norm': 0.0659632682800293, 'learning_rate': 3.6441193238179152e-06, 'epoch': 1.88}
 63%|██████▎   | 200/318 [35:48<21:01, 10.69s/it] 63%|██████▎   | 201/318 [35:59<20:52, 10.70s/it] 64%|██████▎   | 202/318 [36:09<20:14, 10.47s/it] 64%|██████▍   | 203/318 [36:18<19:24, 10.12s/it] 64%|██████▍   | 204/318 [36:28<19:07, 10.06s/it] 64%|██████▍   | 205/318 [36:39<19:27, 10.33s/it] 65%|██████▍   | 206/318 [36:49<19:20, 10.36s/it] 65%|██████▌   | 207/318 [37:00<19:17, 10.43s/it] 65%|██████▌   | 208/318 [37:11<19:21, 10.56s/it] 66%|██████▌   | 209/318 [37:21<19:03, 10.49s/it] 66%|██████▌   | 210/318 [37:30<18:04, 10.04s/it]                                                 {'loss': 0.2105, 'grad_norm': 0.06460994482040405, 'learning_rate': 3.1247039819521907e-06, 'epoch': 1.97}
 66%|██████▌   | 210/318 [37:30<18:04, 10.04s/it] 66%|██████▋   | 211/318 [37:43<19:20, 10.84s/it] 67%|██████▋   | 212/318 [37:53<18:50, 10.67s/it] 67%|██████▋   | 213/318 [38:03<18:14, 10.42s/it] 67%|██████▋   | 214/318 [38:13<18:00, 10.39s/it] 68%|██████▊   | 215/318 [38:23<17:20, 10.10s/it] 68%|██████▊   | 216/318 [38:34<17:51, 10.51s/it] 68%|██████▊   | 217/318 [38:44<17:28, 10.38s/it] 69%|██████▊   | 218/318 [38:54<17:11, 10.31s/it] 69%|██████▉   | 219/318 [39:05<16:59, 10.30s/it] 69%|██████▉   | 220/318 [39:14<16:38, 10.19s/it]                                                 {'loss': 0.2208, 'grad_norm': 0.06648330390453339, 'learning_rate': 2.6278934458271998e-06, 'epoch': 2.07}
 69%|██████▉   | 220/318 [39:14<16:38, 10.19s/it] 69%|██████▉   | 221/318 [39:27<17:21, 10.74s/it] 70%|██████▉   | 222/318 [39:38<17:29, 10.93s/it] 70%|███████   | 223/318 [39:50<17:54, 11.31s/it] 70%|███████   | 224/318 [40:01<17:32, 11.20s/it] 71%|███████   | 225/318 [40:10<16:30, 10.65s/it] 71%|███████   | 226/318 [40:22<16:35, 10.83s/it] 71%|███████▏  | 227/318 [40:30<15:31, 10.23s/it] 72%|███████▏  | 228/318 [40:42<15:51, 10.57s/it] 72%|███████▏  | 229/318 [40:52<15:34, 10.50s/it] 72%|███████▏  | 230/318 [41:05<16:21, 11.15s/it]                                                 {'loss': 0.2075, 'grad_norm': 0.05484795197844505, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.16}
 72%|███████▏  | 230/318 [41:05<16:21, 11.15s/it] 73%|███████▎  | 231/318 [41:17<16:29, 11.38s/it] 73%|███████▎  | 232/318 [41:28<16:14, 11.33s/it] 73%|███████▎  | 233/318 [41:39<16:04, 11.35s/it] 74%|███████▎  | 234/318 [41:50<15:29, 11.06s/it] 74%|███████▍  | 235/318 [41:59<14:41, 10.62s/it] 74%|███████▍  | 236/318 [42:10<14:20, 10.49s/it] 75%|███████▍  | 237/318 [42:20<14:02, 10.40s/it] 75%|███████▍  | 238/318 [42:32<14:45, 11.07s/it] 75%|███████▌  | 239/318 [42:42<14:05, 10.71s/it] 75%|███████▌  | 240/318 [42:53<13:47, 10.61s/it]                                                 {'loss': 0.1869, 'grad_norm': 0.05194808170199394, 'learning_rate': 1.7256963302735752e-06, 'epoch': 2.25}
 75%|███████▌  | 240/318 [42:53<13:47, 10.61s/it] 76%|███████▌  | 241/318 [43:04<13:44, 10.71s/it] 76%|███████▌  | 242/318 [43:14<13:22, 10.56s/it] 76%|███████▋  | 243/318 [43:25<13:33, 10.84s/it] 77%|███████▋  | 244/318 [43:35<13:05, 10.62s/it] 77%|███████▋  | 245/318 [43:45<12:43, 10.46s/it] 77%|███████▋  | 246/318 [43:56<12:45, 10.63s/it] 78%|███████▊  | 247/318 [44:08<13:02, 11.03s/it] 78%|███████▊  | 248/318 [44:19<12:36, 10.81s/it] 78%|███████▊  | 249/318 [44:28<12:03, 10.48s/it] 79%|███████▊  | 250/318 [44:41<12:37, 11.13s/it]                                                 {'loss': 0.2107, 'grad_norm': 0.05950361490249634, 'learning_rate': 1.3311848288809815e-06, 'epoch': 2.35}
 79%|███████▊  | 250/318 [44:41<12:37, 11.13s/it] 79%|███████▉  | 251/318 [44:50<11:33, 10.35s/it] 79%|███████▉  | 252/318 [44:58<10:35,  9.63s/it] 80%|███████▉  | 253/318 [45:08<10:36,  9.79s/it] 80%|███████▉  | 254/318 [45:17<10:20,  9.70s/it] 80%|████████  | 255/318 [45:27<10:20,  9.85s/it] 81%|████████  | 256/318 [45:38<10:19,  9.99s/it] 81%|████████  | 257/318 [45:48<10:17, 10.12s/it] 81%|████████  | 258/318 [46:00<10:39, 10.65s/it] 81%|████████▏ | 259/318 [46:09<10:05, 10.27s/it] 82%|████████▏ | 260/318 [46:20<10:07, 10.48s/it]                                                 {'loss': 0.1938, 'grad_norm': 0.06675506383180618, 'learning_rate': 9.808972011828055e-07, 'epoch': 2.44}
 82%|████████▏ | 260/318 [46:20<10:07, 10.48s/it] 82%|████████▏ | 261/318 [46:31<09:54, 10.43s/it] 82%|████████▏ | 262/318 [46:40<09:26, 10.12s/it] 83%|████████▎ | 263/318 [46:53<09:59, 10.89s/it] 83%|████████▎ | 264/318 [47:03<09:29, 10.55s/it] 83%|████████▎ | 265/318 [47:13<09:18, 10.55s/it] 84%|████████▎ | 266/318 [47:24<09:09, 10.57s/it] 84%|████████▍ | 267/318 [47:34<08:59, 10.57s/it] 84%|████████▍ | 268/318 [47:46<08:58, 10.77s/it] 85%|████████▍ | 269/318 [47:56<08:38, 10.59s/it] 85%|████████▍ | 270/318 [48:07<08:45, 10.95s/it]                                                 {'loss': 0.1993, 'grad_norm': 0.05215850844979286, 'learning_rate': 6.790558119157597e-07, 'epoch': 2.54}
 85%|████████▍ | 270/318 [48:08<08:45, 10.95s/it] 85%|████████▌ | 271/318 [48:20<08:54, 11.38s/it] 86%|████████▌ | 272/318 [48:30<08:31, 11.12s/it] 86%|████████▌ | 273/318 [48:41<08:12, 10.94s/it] 86%|████████▌ | 274/318 [48:52<08:07, 11.09s/it] 86%|████████▋ | 275/318 [49:02<07:39, 10.70s/it] 87%|████████▋ | 276/318 [49:15<07:53, 11.27s/it] 87%|████████▋ | 277/318 [49:26<07:43, 11.32s/it] 87%|████████▋ | 278/318 [49:37<07:30, 11.27s/it] 88%|████████▊ | 279/318 [49:49<07:19, 11.27s/it] 88%|████████▊ | 280/318 [50:01<07:22, 11.65s/it]                                                 {'loss': 0.1959, 'grad_norm': 0.05434335395693779, 'learning_rate': 4.2929905518041714e-07, 'epoch': 2.63}
 88%|████████▊ | 280/318 [50:01<07:22, 11.65s/it] 88%|████████▊ | 281/318 [50:10<06:43, 10.89s/it] 89%|████████▊ | 282/318 [50:19<06:11, 10.33s/it] 89%|████████▉ | 283/318 [50:29<05:57, 10.21s/it] 89%|████████▉ | 284/318 [50:40<05:57, 10.52s/it] 90%|████████▉ | 285/318 [50:50<05:41, 10.36s/it] 90%|████████▉ | 286/318 [51:03<05:52, 11.03s/it] 90%|█████████ | 287/318 [51:13<05:32, 10.73s/it] 91%|█████████ | 288/318 [51:24<05:24, 10.83s/it] 91%|█████████ | 289/318 [51:34<05:03, 10.46s/it] 91%|█████████ | 290/318 [51:45<04:56, 10.60s/it]                                                 {'loss': 0.1914, 'grad_norm': 0.056231722235679626, 'learning_rate': 2.3463749726290287e-07, 'epoch': 2.72}
 91%|█████████ | 290/318 [51:45<04:56, 10.60s/it] 92%|█████████▏| 291/318 [51:54<04:36, 10.25s/it] 92%|█████████▏| 292/318 [52:04<04:22, 10.09s/it] 92%|█████████▏| 293/318 [52:14<04:13, 10.16s/it] 92%|█████████▏| 294/318 [52:25<04:05, 10.24s/it] 93%|█████████▎| 295/318 [52:36<04:02, 10.53s/it] 93%|█████████▎| 296/318 [52:46<03:47, 10.33s/it] 93%|█████████▎| 297/318 [52:57<03:43, 10.65s/it] 94%|█████████▎| 298/318 [53:08<03:32, 10.62s/it] 94%|█████████▍| 299/318 [53:17<03:15, 10.28s/it] 94%|█████████▍| 300/318 [53:27<03:03, 10.18s/it]                                                 {'loss': 0.1851, 'grad_norm': 0.05472579970955849, 'learning_rate': 9.741758728888218e-08, 'epoch': 2.82}
 94%|█████████▍| 300/318 [53:27<03:03, 10.18s/it] 95%|█████████▍| 301/318 [53:40<03:05, 10.92s/it] 95%|█████████▍| 302/318 [53:52<03:02, 11.39s/it] 95%|█████████▌| 303/318 [54:05<02:56, 11.77s/it] 96%|█████████▌| 304/318 [54:15<02:37, 11.22s/it] 96%|█████████▌| 305/318 [54:27<02:31, 11.63s/it] 96%|█████████▌| 306/318 [54:38<02:17, 11.48s/it] 97%|█████████▋| 307/318 [54:50<02:07, 11.58s/it] 97%|█████████▋| 308/318 [55:03<01:58, 11.89s/it] 97%|█████████▋| 309/318 [55:13<01:41, 11.29s/it] 97%|█████████▋| 310/318 [55:25<01:33, 11.68s/it]                                                 {'loss': 0.2002, 'grad_norm': 0.0524914376437664, 'learning_rate': 1.9293373141394124e-08, 'epoch': 2.91}
 97%|█████████▋| 310/318 [55:25<01:33, 11.68s/it] 98%|█████████▊| 311/318 [55:38<01:23, 11.97s/it] 98%|█████████▊| 312/318 [55:47<01:07, 11.24s/it] 98%|█████████▊| 313/318 [55:58<00:55, 11.03s/it] 99%|█████████▊| 314/318 [56:11<00:46, 11.52s/it] 99%|█████████▉| 315/318 [56:20<00:32, 10.82s/it] 99%|█████████▉| 316/318 [56:28<00:20, 10.03s/it]100%|█████████▉| 317/318 [56:39<00:10, 10.23s/it]100%|██████████| 318/318 [56:48<00:00, 10.05s/it][INFO|trainer.py:3801] 2024-11-27 14:35:35,191 >> Saving model checkpoint to /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/checkpoint-318
[INFO|configuration_utils.py:677] 2024-11-27 14:35:35,261 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/config.json
[INFO|configuration_utils.py:746] 2024-11-27 14:35:35,262 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2024-11-27 14:35:35,763 >> tokenizer config file saved in /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/checkpoint-318/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2024-11-27 14:35:35,771 >> Special tokens file saved in /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/checkpoint-318/special_tokens_map.json
[INFO|trainer.py:2584] 2024-11-27 14:35:37,026 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 3410.7654, 'train_samples_per_second': 5.975, 'train_steps_per_second': 0.093, 'train_loss': 0.2671928739397781, 'epoch': 2.99}
100%|██████████| 318/318 [56:50<00:00, 10.05s/it]100%|██████████| 318/318 [56:50<00:00, 10.73s/it]
[INFO|trainer.py:3801] 2024-11-27 14:35:37,039 >> Saving model checkpoint to /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru
[INFO|configuration_utils.py:677] 2024-11-27 14:35:37,090 >> loading configuration file /mnt/afs/hfmodel/Qwen2.5-Coder-7B-Instruct/config.json
[INFO|configuration_utils.py:746] 2024-11-27 14:35:37,091 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2024-11-27 14:35:37,578 >> tokenizer config file saved in /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2024-11-27 14:35:37,587 >> Special tokens file saved in /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/special_tokens_map.json
***** train metrics *****
  epoch                    =       2.9859
  total_flos               = 2209892092GF
  train_loss               =       0.2672
  train_runtime            =   0:56:50.76
  train_samples_per_second =        5.975
  train_steps_per_second   =        0.093
Figure saved at: /mnt/afs/ljl/triton_exps/qwen2.5_coder_7b-instru/synthe+crawl_triton_all_complexInstru/training_loss.png
[WARNING|2024-11-27 14:35:38] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.
[WARNING|2024-11-27 14:35:38] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2024-11-27 14:35:38,061 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2024-11-27 14:35:38,061 >>   Num examples = 755
[INFO|trainer.py:4122] 2024-11-27 14:35:38,061 >>   Batch size = 1
  0%|          | 0/95 [00:00<?, ?it/s]  2%|▏         | 2/95 [00:00<00:19,  4.75it/s]  3%|▎         | 3/95 [00:00<00:22,  4.05it/s]  4%|▍         | 4/95 [00:01<00:28,  3.24it/s]  5%|▌         | 5/95 [00:01<00:27,  3.24it/s]  6%|▋         | 6/95 [00:01<00:30,  2.91it/s]  7%|▋         | 7/95 [00:02<00:29,  2.99it/s]  8%|▊         | 8/95 [00:02<00:31,  2.76it/s]  9%|▉         | 9/95 [00:03<00:32,  2.63it/s] 11%|█         | 10/95 [00:03<00:30,  2.79it/s] 12%|█▏        | 11/95 [00:03<00:27,  3.05it/s] 13%|█▎        | 12/95 [00:03<00:27,  3.06it/s] 14%|█▎        | 13/95 [00:04<00:25,  3.18it/s] 15%|█▍        | 14/95 [00:04<00:24,  3.27it/s] 16%|█▌        | 15/95 [00:04<00:24,  3.33it/s] 17%|█▋        | 16/95 [00:05<00:26,  2.98it/s] 18%|█▊        | 17/95 [00:05<00:24,  3.13it/s] 19%|█▉        | 18/95 [00:05<00:22,  3.37it/s] 20%|██        | 19/95 [00:06<00:25,  3.00it/s] 21%|██        | 20/95 [00:06<00:22,  3.28it/s] 22%|██▏       | 21/95 [00:06<00:23,  3.09it/s] 23%|██▎       | 22/95 [00:07<00:25,  2.91it/s] 24%|██▍       | 23/95 [00:07<00:23,  3.01it/s] 25%|██▌       | 24/95 [00:07<00:23,  3.03it/s] 26%|██▋       | 25/95 [00:08<00:23,  2.94it/s] 27%|██▋       | 26/95 [00:08<00:23,  3.00it/s] 28%|██▊       | 27/95 [00:08<00:23,  2.85it/s] 29%|██▉       | 28/95 [00:09<00:25,  2.58it/s] 31%|███       | 29/95 [00:09<00:23,  2.76it/s] 32%|███▏      | 30/95 [00:09<00:21,  2.99it/s] 33%|███▎      | 31/95 [00:10<00:20,  3.09it/s] 34%|███▎      | 32/95 [00:10<00:20,  3.13it/s] 35%|███▍      | 33/95 [00:10<00:21,  2.89it/s] 36%|███▌      | 34/95 [00:11<00:22,  2.72it/s] 37%|███▋      | 35/95 [00:11<00:22,  2.63it/s] 38%|███▊      | 36/95 [00:12<00:21,  2.70it/s] 39%|███▉      | 37/95 [00:12<00:20,  2.88it/s] 40%|████      | 38/95 [00:12<00:18,  3.13it/s] 41%|████      | 39/95 [00:13<00:19,  2.86it/s] 42%|████▏     | 40/95 [00:13<00:17,  3.11it/s] 43%|████▎     | 41/95 [00:13<00:17,  3.14it/s] 44%|████▍     | 42/95 [00:13<00:16,  3.30it/s] 45%|████▌     | 43/95 [00:14<00:17,  2.95it/s] 46%|████▋     | 44/95 [00:14<00:16,  3.12it/s] 47%|████▋     | 45/95 [00:14<00:17,  2.85it/s] 48%|████▊     | 46/95 [00:15<00:16,  3.03it/s] 49%|████▉     | 47/95 [00:15<00:16,  2.99it/s] 51%|█████     | 48/95 [00:15<00:15,  3.06it/s] 52%|█████▏    | 49/95 [00:16<00:16,  2.82it/s] 53%|█████▎    | 50/95 [00:16<00:14,  3.10it/s] 54%|█████▎    | 51/95 [00:16<00:15,  2.91it/s] 55%|█████▍    | 52/95 [00:17<00:14,  3.06it/s] 56%|█████▌    | 53/95 [00:17<00:14,  2.96it/s] 57%|█████▋    | 54/95 [00:17<00:12,  3.23it/s] 58%|█████▊    | 55/95 [00:18<00:13,  2.91it/s] 59%|█████▉    | 56/95 [00:18<00:13,  2.82it/s] 60%|██████    | 57/95 [00:18<00:12,  3.16it/s] 61%|██████    | 58/95 [00:19<00:11,  3.16it/s] 62%|██████▏   | 59/95 [00:19<00:11,  3.07it/s] 63%|██████▎   | 60/95 [00:19<00:11,  3.16it/s] 64%|██████▍   | 61/95 [00:20<00:11,  2.95it/s] 65%|██████▌   | 62/95 [00:20<00:10,  3.07it/s] 66%|██████▋   | 63/95 [00:20<00:10,  3.14it/s] 67%|██████▋   | 64/95 [00:21<00:09,  3.35it/s] 68%|██████▊   | 65/95 [00:21<00:09,  3.13it/s] 69%|██████▉   | 66/95 [00:21<00:08,  3.30it/s] 71%|███████   | 67/95 [00:22<00:09,  2.96it/s] 72%|███████▏  | 68/95 [00:22<00:08,  3.07it/s] 73%|███████▎  | 69/95 [00:22<00:07,  3.33it/s] 74%|███████▎  | 70/95 [00:23<00:08,  2.82it/s] 75%|███████▍  | 71/95 [00:23<00:08,  2.68it/s] 76%|███████▌  | 72/95 [00:23<00:08,  2.70it/s] 77%|███████▋  | 73/95 [00:24<00:08,  2.48it/s] 78%|███████▊  | 74/95 [00:24<00:08,  2.44it/s] 79%|███████▉  | 75/95 [00:25<00:08,  2.32it/s] 80%|████████  | 76/95 [00:25<00:07,  2.56it/s] 81%|████████  | 77/95 [00:25<00:06,  2.79it/s] 82%|████████▏ | 78/95 [00:26<00:06,  2.66it/s] 83%|████████▎ | 79/95 [00:26<00:05,  2.78it/s] 84%|████████▍ | 80/95 [00:27<00:05,  2.64it/s] 85%|████████▌ | 81/95 [00:27<00:05,  2.65it/s] 86%|████████▋ | 82/95 [00:27<00:04,  2.86it/s] 87%|████████▋ | 83/95 [00:28<00:03,  3.07it/s] 88%|████████▊ | 84/95 [00:28<00:03,  3.23it/s] 89%|████████▉ | 85/95 [00:28<00:03,  3.00it/s] 91%|█████████ | 86/95 [00:28<00:02,  3.04it/s] 92%|█████████▏| 87/95 [00:29<00:02,  2.80it/s] 93%|█████████▎| 88/95 [00:29<00:02,  3.06it/s] 94%|█████████▎| 89/95 [00:29<00:01,  3.29it/s] 95%|█████████▍| 90/95 [00:30<00:01,  3.48it/s] 96%|█████████▌| 91/95 [00:30<00:01,  3.39it/s] 97%|█████████▋| 92/95 [00:30<00:00,  3.48it/s] 98%|█████████▊| 93/95 [00:31<00:00,  3.06it/s] 99%|█████████▉| 94/95 [00:31<00:00,  2.83it/s]100%|██████████| 95/95 [00:31<00:00,  2.73it/s]100%|██████████| 95/95 [00:31<00:00,  2.97it/s]
***** eval metrics *****
  epoch                   =     2.9859
  eval_loss               =     0.2417
  eval_runtime            = 0:00:32.27
  eval_samples_per_second =     23.391
  eval_steps_per_second   =      2.943
[INFO|modelcard.py:449] 2024-11-27 14:36:10,372 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W1127 14:36:10.856149936 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
